{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eWhmZwqpba0v"
   },
   "source": [
    "# Text Mining-tokenization\n",
    "data source: wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y_eaR7jOdhCa"
   },
   "source": [
    "## Loading Wikipedia data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8PphW5zZb7nI",
    "outputId": "4733cfc9-7369-4844-a410-96b0f5b8334b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wikipedia\n",
      "  Using cached wikipedia-1.4.0-py3-none-any.whl\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\user\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from wikipedia) (4.11.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in c:\\users\\user\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from wikipedia) (2.27.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\user\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\user\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.0.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2021.10.8)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\user\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from beautifulsoup4->wikipedia) (2.3.1)\n",
      "Installing collected packages: wikipedia\n",
      "Successfully installed wikipedia-1.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "IhhqcQ5ra7r-"
   },
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MACErP6Ub1yw",
    "outputId": "44325f99-2f9f-4ebc-9ba9-2b32d8de1181"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://en.wikipedia.org/wiki/Taipei\n",
      "<class 'str'>\n",
      "The length of Taipei page is:  46184\n"
     ]
    }
   ],
   "source": [
    "# page \"Taipei\"\n",
    "cv = wikipedia.page(\"Taipei\")\n",
    "text = cv.content\n",
    "print(cv.url)\n",
    "print(type(text))\n",
    "print(\"The length of Taipei page is: \", len(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CZotBYrRczMe",
    "outputId": "3b760524-c231-421d-c3a8-f48c3cc8da0c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1832\n",
      "The length of Taipei summary is:  1832\n"
     ]
    }
   ],
   "source": [
    "# summary\n",
    "summary_text = wikipedia.summary(\"Taipei\", sentences = 20)\n",
    "print(len(summary_text))\n",
    "print(\"The length of Taipei summary is: \", len(summary_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "S2cXPTEVd5sH"
   },
   "outputs": [],
   "source": [
    "# print(summary_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZoCtGJ4-e5fy"
   },
   "source": [
    "## Tokenization\n",
    "\n",
    "split strings, sentences into a list of tokens\n",
    "*   Method1: `.split()`\n",
    "*   Method2: `nltk`\n",
    "*   Method3: python function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k-OOh5OYfJC3"
   },
   "source": [
    "### Method1： `.split()`\n",
    "以空格區隔句子(僅英文文本適用)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3B2YQMoYHP-a",
    "outputId": "215476ed-d1a9-45e9-fe62-8987077bb749"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Taipei is the economic, political, educational and cultural center of Taiwan and one of the major hubs in East Asia.']\n",
      "['2022', '08', '16 00:10:32']\n"
     ]
    }
   ],
   "source": [
    "# example\n",
    "sentence_a = \"Taipei is the economic, political, educational and cultural center of Taiwan and one of the major hubs in East Asia.\"\n",
    "print(sentence_a.split(\"　\"))\n",
    "\n",
    "sentence_b = \"2022/08/16 00:10:32\"\n",
    "print(sentence_b.split(\"/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HGSpEcy3fi6U",
    "outputId": "d0b62a72-43f1-4be7-888a-0cc6b87b17af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7086\n",
      "['Taipei', '(),', 'officially', 'Taipei', 'City,', 'is', 'the', 'capital', 'and', 'a', 'special', 'municipality', 'of', 'the', 'Republic', 'of', 'China', '(Taiwan).', 'Located', 'in', 'Northern', 'Taiwan,', 'Taipei', 'City', 'is', 'an', 'enclave', 'of', 'the', 'municipality', 'of', 'New', 'Taipei', 'City', 'that', 'sits', 'about', '25', 'km', '(16', 'mi)', 'southwest', 'of', 'the', 'northern', 'port', 'city', 'of', 'Keelung.', 'Most']\n"
     ]
    }
   ],
   "source": [
    "print(len(text.split(\" \")))\n",
    "print(text.split(\" \")[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dXtxE2Nvgm2F"
   },
   "source": [
    "### Method2： by nltk's function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "     ---------------------------------------- 1.5/1.5 MB 3.4 MB/s eta 0:00:00\n",
      "Collecting click\n",
      "  Downloading click-8.1.3-py3-none-any.whl (96 kB)\n",
      "     ---------------------------------------- 96.6/96.6 KB 1.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: joblib in c:\\users\\user\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Collecting regex>=2021.8.3\n",
      "  Downloading regex-2023.5.4-cp38-cp38-win_amd64.whl (267 kB)\n",
      "     -------------------------------------- 267.9/267.9 KB 4.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from nltk) (4.64.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from click->nltk) (0.4.4)\n",
      "Installing collected packages: regex, click, nltk\n",
      "Successfully installed click-8.1.3 nltk-3.8.1 regex-2023.5.4\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V7jWUr23gNEz",
    "outputId": "3aa37b30-08cb-4064-a479-66d00afe67d7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8334\n",
      "['Taipei', '(', ')', ',', 'officially', 'Taipei', 'City', ',', 'is', 'the', 'capital', 'and', 'a', 'special', 'municipality', 'of', 'the', 'Republic', 'of', 'China', '(', 'Taiwan', ')', '.', 'Located', 'in', 'Northern', 'Taiwan', ',', 'Taipei', 'City', 'is', 'an', 'enclave', 'of', 'the', 'municipality', 'of', 'New', 'Taipei', 'City', 'that', 'sits', 'about', '25', 'km', '(', '16', 'mi', ')']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk_tokenize = word_tokenize(text)\n",
    "print(len(nltk_tokenize))\n",
    "print(nltk_tokenize[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NXp9VMHbiLaV"
   },
   "source": [
    "### Method3: by python funcior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z9SD8vP7h9c9",
    "outputId": "88e97887-8381-410d-8867-bacf52953124"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7085\n",
      "['Taipei', '(),', 'officially', 'Taipei', 'City,', 'is', 'the', 'capital', 'and', 'a', 'special', 'municipality', 'of', 'the', 'Republic', 'of', 'China', '(Taiwan).', 'Located', 'in', 'Northern', 'Taiwan,', 'Taipei', 'City', 'is', 'an', 'enclave', 'of', 'the', 'municipality', 'of', 'New', 'Taipei', 'City', 'that', 'sits', 'about', '25', 'km', '(16', 'mi)', 'southwest', 'of', 'the', 'northern', 'port', 'city', 'of', 'Keelung.', 'Most']\n"
     ]
    }
   ],
   "source": [
    "def tokenizer(txt):\n",
    "  tok = \"\"\n",
    "  word_list = []\n",
    "\n",
    "  for ch in txt:\n",
    "    # print(ch)\n",
    "    if ch == \" \":\n",
    "      word_list.append(tok)\n",
    "      tok = \"\"\n",
    "    else:\n",
    "      tok += ch\n",
    "  return word_list\n",
    "\n",
    "word_list = tokenizer(text)\n",
    "print(len(word_list))\n",
    "print(word_list[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zLdo9dP2kXOm"
   },
   "source": [
    "## Counting\n",
    "計算每個字詞出現次數\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JK5TS_hJkaf5",
    "outputId": "01f1ee7d-6557-4384-edd2-b8907e4a7c58"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\t451\n",
      "of\t248\n",
      "and\t237\n",
      "in\t206\n",
      "Taipei\t116\n",
      "to\t113\n",
      "is\t101\n",
      "a\t98\n",
      "The\t83\n",
      "as\t75\n",
      "by\t54\n",
      "was\t46\n",
      "city\t41\n",
      "on\t41\n",
      "for\t41\n",
      "are\t40\n",
      "Taiwan\t37\n",
      "from\t35\n",
      "has\t34\n",
      "with\t33\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "word_count = Counter(word_list)\n",
    "for w, c in word_count.most_common(20):\n",
    "  print(\"%s\\t%s\" % (w, c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bI7Fld80mhf8"
   },
   "source": [
    "## Stopword and sign removal\n",
    "使用Tokenization method2 nltk斷詞後的結果(nltk_tokenize)\n",
    "\n",
    "\n",
    "**移除標點符號**\n",
    "* Method1: `string` using `string.punctuation`\n",
    "* Method2: `.isalpha()` 去除所有包含非字母的字詞\n",
    "\n",
    "**移除stopwords**\n",
    "* load english stopwords from NLTK\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4yohA74lm_Ax"
   },
   "source": [
    "### Remove punctuation Method 1: by `string`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "L00f4UAbnpLa"
   },
   "outputs": [],
   "source": [
    "tokens = nltk_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tcAb--EklC0q",
    "outputId": "8f8de54e-3324-4380-c4fc-484223eb70c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I5wkcmYjmor7",
    "outputId": "5e8d4b48-ddb4-49a5-9c80-cf6b4591e180"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7336\n",
      "['Taipei', 'officially', 'Taipei', 'City', 'is', 'the', 'capital', 'and', 'a', 'special', 'municipality', 'of', 'the', 'Republic', 'of', 'China', 'Taiwan', 'Located', 'in', 'Northern', 'Taiwan', 'Taipei', 'City', 'is', 'an', 'enclave', 'of', 'the', 'municipality', 'of', 'New', 'Taipei', 'City', 'that', 'sits', 'about', '25', 'km', '16', 'mi', 'southwest', 'of', 'the', 'northern', 'port', 'city', 'of', 'Keelung', 'Most', 'of']\n"
     ]
    }
   ],
   "source": [
    "def remove_punc(tokens):\n",
    "  tokens_list = []\n",
    "  for tok in tokens:\n",
    "    if tok not in string.punctuation:\n",
    "      tokens_list.append(tok)\n",
    "  return tokens_list\n",
    "\n",
    "print(len(remove_punc(tokens)))\n",
    "print(remove_punc(tokens)[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GpGHeeEEJOfS",
    "outputId": "56d3a4b9-ff63-4d70-e9a1-364b28e99658"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "移除標點符號前\n",
      "the\t454\n",
      ",\t446\n",
      ".\t304\n",
      "of\t249\n",
      "and\t237\n",
      "in\t206\n",
      "Taipei\t172\n",
      "to\t113\n",
      "is\t101\n",
      "(\t98\n",
      "=========================\n",
      "移除標點符號後\n",
      "the\t454\n",
      "of\t249\n",
      "and\t237\n",
      "in\t206\n",
      "Taipei\t172\n",
      "to\t113\n",
      "is\t101\n",
      "a\t98\n",
      "The\t94\n",
      "as\t75\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "print(\"移除標點符號前\")\n",
    "tokens = nltk_tokenize\n",
    "word_count = Counter(tokens)\n",
    "for w, c in word_count.most_common(10):\n",
    "  print(\"%s\\t%d\" % (w, c))\n",
    "\n",
    "print(\"=========================\")\n",
    "\n",
    "print(\"移除標點符號後\")\n",
    "tokens = remove_punc(tokens)\n",
    "word_count = Counter(tokens)\n",
    "for w, c in word_count.most_common(10):\n",
    "  print(\"%s\\t%d\" % (w, c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jKrlXNREo3kP"
   },
   "source": [
    "### Remove punctuation Method2: `.isalpha()` \n",
    "去除所有包含非英文的字詞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "hX1M2R26qKF2"
   },
   "outputs": [],
   "source": [
    "tokens = nltk_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DS3SuORinTEk",
    "outputId": "c0939bc2-1828-453f-a209-bf1b77a68dab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6714\n",
      "['Taipei', 'officially', 'Taipei', 'City', 'is', 'the', 'capital', 'and', 'a', 'special', 'municipality', 'of', 'the', 'Republic', 'of', 'China', 'Taiwan', 'Located', 'in', 'Northern', 'Taiwan', 'Taipei', 'City', 'is', 'an', 'enclave', 'of', 'the', 'municipality', 'of', 'New', 'Taipei', 'City', 'that', 'sits', 'about', 'km', 'mi', 'southwest', 'of', 'the', 'northern', 'port', 'city', 'of', 'Keelung', 'Most', 'of', 'the', 'city']\n"
     ]
    }
   ],
   "source": [
    "def remove_punc2(tokens):\n",
    "  tokens_list = []\n",
    "  for tok in tokens:\n",
    "    if tok.isalpha():\n",
    "      tokens_list.append(tok)\n",
    "  return tokens_list\n",
    "\n",
    "# def remove_punc_short(tokens):\n",
    "  # return [tok for tok in tokens if tok.isalpha()]\n",
    "\n",
    "print(len(remove_punc2(tokens)))\n",
    "print(remove_punc2(tokens)[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N7CtPkPNrttf"
   },
   "source": [
    "### Remove Stopwords\n",
    "載入NLTK套件中的英文stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "OkJ-hU_tuAHH"
   },
   "outputs": [],
   "source": [
    "tokens = nltk_tokenize\n",
    "tokens = remove_punc2(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XHglj7YurTbW",
    "outputId": "f000eccf-44ba-4764-dde4-256e61e5d6f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# load english stopwords from NLTK\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "stopword_list = stopwords.words('english')\n",
    "\n",
    "print(stopword_list[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r5yiYsJCtE_h",
    "outputId": "a9e7e5a5-59c3-4358-9f71-f080fb033a25"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4411\n",
      "['Taipei', 'officially', 'Taipei', 'City', 'capital', 'special', 'municipality', 'Republic', 'China', 'Taiwan', 'Located', 'Northern', 'Taiwan', 'Taipei', 'City', 'enclave', 'municipality', 'New', 'Taipei', 'City', 'sits', 'km', 'mi', 'southwest', 'northern', 'port', 'city', 'Keelung', 'Most', 'city', 'rests', 'Taipei', 'Basin', 'ancient', 'lakebed', 'The', 'basin', 'bounded', 'relatively', 'narrow', 'valleys', 'Keelung', 'Xindian', 'rivers', 'join', 'form', 'Tamsui', 'River', 'along', 'city']\n"
     ]
    }
   ],
   "source": [
    "# remove\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "  tokens_list = []\n",
    "  for tok in tokens:\n",
    "    if tok not in stopword_list:\n",
    "      tokens_list.append(tok)\n",
    "  return tokens_list\n",
    "\n",
    "print(len(remove_stopwords(tokens)))\n",
    "print(remove_stopwords(tokens)[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PaHZsjWuti6P",
    "outputId": "08f7cbb2-eae0-4955-b535-ec56e5c42457"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taipei\t172\n",
      "The\t94\n",
      "Taiwan\t71\n",
      "city\t61\n",
      "City\t26\n",
      "also\t23\n",
      "District\t22\n",
      "area\t21\n",
      "Japanese\t20\n",
      "population\t19\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "tokens = remove_stopwords(tokens)\n",
    "word_count = Counter(tokens)\n",
    "for w, c in word_count.most_common(10):\n",
    "  print(\"%s\\t%d\" % (w, c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hExff6jXGF7o"
   },
   "source": [
    "## Stemming and Lemmatization\n",
    "* 處理英文文本中，同一個單詞的拼法上可能隨著時態、單複數、主被動而不同之問題\n",
    "* 將不同的形態歸一化，降低複雜度同時加快語言模型訓練速度\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "1.  **Stemming** ：Snowball algorithm\n",
    "2.  **Lemmatization** \n",
    "：還原字的原型(通常較stemming精準)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ewOedXw-vVCn"
   },
   "source": [
    "### Stemming: Snowball algorithm\n",
    "\n",
    "Reference: http://snowball.tartarus.org/texts/introduction.html\n",
    "\n",
    "`from nltk.stem.snowball import SnowballStemmer`\n",
    "\n",
    "缺點：可能會有overstemming的問題"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "a1HDlQOGv4P0"
   },
   "outputs": [],
   "source": [
    "tokens = remove_stopwords(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g3djuv0mtwmM",
    "outputId": "09158261-644e-4b85-b779-9e1a1c944926"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "taipei\t172\n",
      "citi\t102\n",
      "the\t94\n",
      "taiwan\t71\n",
      "district\t42\n",
      "area\t28\n",
      "includ\t26\n",
      "nation\t26\n",
      "world\t26\n",
      "templ\t26\n",
      "intern\t26\n",
      "new\t24\n",
      "also\t23\n",
      "popul\t22\n",
      "build\t21\n",
      "locat\t20\n",
      "center\t20\n",
      "japanes\t20\n",
      "museum\t20\n",
      "system\t20\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "snowball_stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "stemmed_tokens = []\n",
    "for tok in tokens:\n",
    "  stemmed_tokens.append(snowball_stemmer.stem(tok))\n",
    "\n",
    "word_count = Counter(stemmed_tokens)\n",
    "\n",
    "for w, c in word_count.most_common(20):\n",
    "  print(\"%s\\t%d\" % (w, c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fhs-RHeoxvI4"
   },
   "source": [
    "## Lemmatization\n",
    "還原字的原型，精準度比stemming高"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6b7GZlBhxuAZ",
    "outputId": "d34e830d-5ab4-4eb8-9929-d6d57f88dddf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dogs\n",
      "dog\n",
      "hit\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize(token):\n",
    "  # ADJ (a), ADJ_SAT (s), ADV (r), NOUN (n) or VERB (v)\n",
    "  for p in ['v', 'n', 'a', 'r', 's']:\n",
    "    l = wordnet_lemmatizer.lemmatize(token, pos=p)\n",
    "    if l != token:\n",
    "      return l\n",
    "  return token\n",
    "\n",
    "print(lemmatize('Dogs'))\n",
    "print(lemmatize('dogs'))\n",
    "print(lemmatize('hits'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TubAGIdkzJN2"
   },
   "source": [
    "show differences between stemming and lemmatization → lemmatization is better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U0Q_6M11y1GO",
    "outputId": "e6e17c2d-9911-42f2-a272-5ceecc3ab649"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words \t Stemming \t Lemmatization\n",
      "unopened\tunopen\tunopened\n",
      "decompose\tdecompos\tdecompose\n",
      "decomposes\tdecompos\tdecompose\n",
      "decomposed\tdecompos\tdecompose\n",
      "decomposing\tdecompos\tdecompose\n",
      "does\tdoe\tdo\n",
      "did\tdid\tdo\n",
      "wrote\twrote\twrite\n",
      "written\twritten\twrite\n",
      "ran\tran\trun\n",
      "gave\tgave\tgive\n",
      "held\theld\thold\n",
      "went\twent\tgo\n",
      "gone\tgone\tgo\n",
      "lain\tlain\tlie\n",
      "people\tpeopl\tpeople\n",
      "feet\tfeet\tfoot\n",
      "women\twomen\twoman\n",
      "smoothly\tsmooth\tsmoothly\n",
      "firstly\tfirst\tfirstly\n",
      "secondly\tsecond\tsecondly\n"
     ]
    }
   ],
   "source": [
    "print(\"words \\t Stemming \\t Lemmatization\")\n",
    "for w in [\n",
    "    'open', 'opens', 'opened', 'opening', 'unopened',\n",
    "    'talk', 'talks', 'talked', 'talking',\n",
    "    'decompose', 'decomposes', 'decomposed', 'decomposing',\n",
    "    'do', 'does', 'did', \n",
    "    'wrote', 'written', 'ran', 'gave', 'held', 'went', 'gone',\n",
    "    'lied', 'lies', 'lay', 'lain', 'lying', \n",
    "    'cats', 'people', 'feet', 'women', 'smoothly', 'firstly', 'secondly', \n",
    "    ]:\n",
    "    s = snowball_stemmer.stem(w)\n",
    "    l = lemmatize(w)\n",
    "    if s != l:\n",
    "        print(\"%s\\t%s\\t%s\" % (w, s, l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pkAXrleqzTft",
    "outputId": "2e82e1c7-1f83-4ab4-f40f-ed6afffc6782"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taipei\t172\n",
      "The\t94\n",
      "city\t72\n",
      "Taiwan\t71\n",
      "area\t27\n",
      "City\t26\n",
      "include\t26\n",
      "also\t23\n",
      "District\t22\n",
      "Japanese\t20\n",
      "build\t20\n",
      "district\t20\n",
      "population\t19\n",
      "In\t19\n",
      "National\t18\n",
      "Chinese\t18\n",
      "international\t17\n",
      "system\t17\n",
      "know\t15\n",
      "center\t15\n"
     ]
    }
   ],
   "source": [
    "tokens = remove_stopwords(tokens)\n",
    "lemmatized_tokens = []\n",
    "for tok in tokens:\n",
    "    lemmatized_tokens.append(lemmatize(tok))\n",
    "word_count = Counter(lemmatized_tokens)\n",
    "\n",
    "for w, c in word_count.most_common(20):\n",
    "  print(\"%s\\t%d\" % (w, c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2MHt19_Qz3Bb"
   },
   "source": [
    "## Applications: WordClund 文字雲\n",
    "將字串以空白隔開後製作成文字雲\n",
    "https://www.jasondavies.com/wordcloud/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oNDNOFlEz9hf",
    "outputId": "e68576f0-1cf1-46f3-dde2-07be512cf388"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taipei Taipei Taipei Taipei Taipei Taipei Taipei Taipei Taipei Taipei Taipei Taipei Taipei Taipei Ta\n"
     ]
    }
   ],
   "source": [
    "repeated_tokens = []\n",
    "for w, c in word_count.most_common():\n",
    "  for i in range(c):\n",
    "    repeated_tokens.append(w)\n",
    "\n",
    "print(\" \".join(repeated_tokens)[:100])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
